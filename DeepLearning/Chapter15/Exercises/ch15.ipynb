{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_download_root = \"http://download.tensorflow.org/data/\"\n",
    "filename = \"quickdraw_tutorial_dataset_v1.tar.gz\"\n",
    "filepath = tf.keras.utils.get_file(filename,\n",
    "                                   tf_download_root + filename,\n",
    "                                   cache_dir=\".\",\n",
    "                                   extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = Path(filepath).parent\n",
    "\n",
    "trains_files = sorted([str(path) for path in links.glob('training.tfrecord-*')])\n",
    "eval_files = sorted([str(path) for path in links.glob('eval.tfrecord-*')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(links / 'training.tfrecord.classes') as train_class_files:\n",
    "    train_classes = train_class_files.readlines() \n",
    "\n",
    "with open(links / 'eval.tfrecord.classes') as test_class_files:\n",
    "    test_classes = test_class_files.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_classes == test_classes\n",
    "\n",
    "classes_name = [name.strip().lower() for name in train_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(classes_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(data_batch):\n",
    "    feature_description = {\n",
    "        'ink': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'shape': tf.io.FixedLenFeature([2],dtype=tf.int64),\n",
    "        'class_index': tf.io.FixedLenFeature([1], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    examples = tf.io.parse_example(data_batch, feature_description)\n",
    "    flat_sketches = tf.sparse.to_dense(examples[\"ink\"])\n",
    "    sketches = tf.reshape(flat_sketches, shape=[tf.size(data_batch), -1, 3])\n",
    "    lengths = examples['shape'][:, 0]\n",
    "    labels = examples['class_index'][:, 0]\n",
    "\n",
    "    return sketches, lengths, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_draw(filepath, batch_size=32, shuffle_buffer_size=None, n_parse_threads=5, n_read_threads=5, cache=False):\n",
    "    dataset = tf.data.TFRecordDataset(filepath, num_parallel_reads=n_read_threads)\n",
    "\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    \n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(parse, num_parallel_calls=n_parse_threads)\n",
    "\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = quick_draw(trains_files, shuffle_buffer_size=10000)\n",
    "valid_set = quick_draw(eval_files[:5])\n",
    "test_set = quick_draw(eval_files[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sketch, length ,label, in train_set.take(1):\n",
    "    print('Sketch= ', sketch)\n",
    "    print('Length= ', length)\n",
    "    print('Label= ', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sketch(sketch, label=None):\n",
    "    origin = np.array([[0., 0., 0.]])\n",
    "    sketch = np.r_[origin, sketch]\n",
    "    stroke_end_indices = np.argwhere(sketch[:, -1]==1.)[:, 0]\n",
    "    coordinates = np.cumsum(sketch[:, :2], axis=0)\n",
    "    strokes = np.split(coordinates, stroke_end_indices + 1)\n",
    "    title = classes_name[label.numpy()] if label is not None else \"Try to guess\"\n",
    "    plt.title(title)\n",
    "    plt.plot(coordinates[:, 0], -coordinates[:, 1], \"y:\")\n",
    "    for stroke in strokes:\n",
    "        plt.plot(stroke[:, 0], -stroke[:, 1], \".-\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sketches(sketches, lengths, labels):\n",
    "    n_sketches = len(sketches)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_sketches - 1) // n_cols + 1\n",
    "    plt.figure(figsize=(n_cols * 3, n_rows * 3.5))\n",
    "    for index, sketch, length, label in zip(range(n_sketches), sketches, lengths, labels):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        draw_sketch(sketch[:length], label)\n",
    "    plt.show()\n",
    "\n",
    "for sketches, lengths, labels in train_set.take(1):\n",
    "    draw_sketches(sketches, lengths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_long_sketches(dataset, max_length=100):\n",
    "    return dataset.map(lambda inks, lengths, labels: (inks[:, :max_length], labels))\n",
    "\n",
    "cropped_train_set = crop_long_sketches(train_set)\n",
    "cropped_valid_set = crop_long_sketches(valid_set)\n",
    "cropped_test_set = crop_long_sketches(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(32, kernel_size=5, strides=2, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(64, kernel_size=5, strides=2, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(128, kernel_size=3, strides=2, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(len(classes_name), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, clipnorm=1)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(cropped_train_set, epochs=2,\n",
    "          validation_data=cropped_valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([labels for _, _, labels in test_set])\n",
    "y_probas = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tf.keras.metrics.sparse_top_k_categorical_accuracy(y_test, y_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = 10\n",
    "Y_probas = model.predict(sketches)\n",
    "top_k = tf.nn.top_k(Y_probas, k=5)\n",
    "for index in range(n_new):\n",
    "    plt.figure(figsize=(3, 3.5))\n",
    "    draw_sketch(sketches[index])\n",
    "    plt.show()\n",
    "    print(\"Top-5 predictions:\".format(index + 1))\n",
    "    for k in range(5):\n",
    "        class_name = classes_name[top_k.indices[index, k]]\n",
    "        proba = 100 * top_k.values[index, k]\n",
    "        print(\"  {}. {} {:.3f}%\".format(k + 1, class_name, proba))\n",
    "    print(\"Answer: {}\".format(classes_name[labels[index].numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.get_file(\n",
    "    \"jsb_chorales.tgz\",\n",
    "    \"https://github.com/ageron/data/raw/main/jsb_chorales.tgz\",\n",
    "    cache_dir='.',\n",
    "    extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsb_chorales_dir = Path('datasets/jsb_chorales')\n",
    "\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_set = load_data(train_files)\n",
    "valid_set = load_data(valid_files)\n",
    "test_set = load_data(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_set, valid_set, test_set):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord) # |= mean union\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "def notes_to_frequencies(notes):\n",
    "    return 2 ** ((np.array(notes) - 69) / 12) * 440\n",
    "\n",
    "def frequencies_to_samples(frequencies, tempo, sample_rate):\n",
    "    note_duration = 60 / tempo \n",
    "    frequencies = (note_duration * frequencies).round() / note_duration\n",
    "    n_samples = int(note_duration * sample_rate)\n",
    "    time = np.linspace(0, note_duration, n_samples)\n",
    "    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n",
    "    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n",
    "    return sine_waves.reshape(-1)\n",
    "\n",
    "def chords_to_samples(chords, tempo, sample_rate):\n",
    "    freqs = notes_to_frequencies(chords)\n",
    "    freqs = np.r_[freqs, freqs[-1:]] \n",
    "    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n",
    "                     for melody in freqs.T], axis=0)\n",
    "    n_fade_out_samples = sample_rate * 60 // tempo \n",
    "    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n",
    "    merged[-n_fade_out_samples:] *= fade_out\n",
    "    return merged\n",
    "\n",
    "def play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n",
    "    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n",
    "    if filepath:\n",
    "        from scipy.io import wavfile\n",
    "        samples = (2**15 * samples).astype(np.int16)\n",
    "        wavfile.write(filepath, sample_rate, samples)\n",
    "        return display(Audio(filepath))\n",
    "    else:\n",
    "        return display(Audio(samples, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(3):\n",
    "    play_chords(train_set[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(batch):\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] \n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1)\n",
    "    return tf.reshape(window, [-1]) \n",
    "\n",
    "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                 window_size=32, window_shift=16, cache=True):\n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        return dataset.flat_map(batch_window)\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(create_target)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = bach_dataset(train_set, shuffle_buffer_size=1000)\n",
    "valid_set = bach_dataset(valid_set)\n",
    "test_set = bach_dataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embedding_dims = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims\n",
    "                              ,\n",
    "                           input_shape=[None]),\n",
    "    tf.keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_notes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=20, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
