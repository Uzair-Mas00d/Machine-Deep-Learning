{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_valid, y_valid = X_train_full[:5000], y_train_full[:5000]\n",
    "X_train, y_train = X_train_full[5000:], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28)\n",
      "(55000,)\n",
      "(5000, 28, 28)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tensors.proto\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensors.proto\n",
    "syntax = \"proto3\";\n",
    "\n",
    "message BytesList { repeated bytes value = 1; }\n",
    "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
    "message Feature {\n",
    "    oneof kind {\n",
    "        BytesList bytes_list = 1;\n",
    "        Int64List int64_list = 3;\n",
    "    }\n",
    "};\n",
    "message Features { map<string, Feature> feature = 1; };\n",
    "message Example { Features features = 1; };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!protoc tensors.proto --python_out=. --descriptor_set_out=tensors.desc --include_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensors_pb2 import BytesList, Int64List\n",
    "from tensors_pb2 import Feature, Features, Example\n",
    "\n",
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "\n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\": Feature(int64_list=Int64List(value=[label])),\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_record_new_version(X, y, name, num_files=2):\n",
    "    records_per_files = len(X) // num_files\n",
    "    reminder = len(X) % num_files\n",
    "    paths = []\n",
    "    for i in range(num_files):\n",
    "        file_name = f\"{name}_{i + 1}.tfrecord\"\n",
    "        paths.append(file_name)\n",
    "\n",
    "        start_indx = i * records_per_files\n",
    "        end_indx = (i + 1) * records_per_files if i < num_files - 1 else None\n",
    "\n",
    "        if reminder > 0 and i >= num_files - reminder:\n",
    "            end_indx += 1\n",
    "\n",
    "        X_i = X[start_indx:end_indx]\n",
    "        y_i = y[start_indx:end_indx]\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_i, y_i))\n",
    "        if name == \"train\":\n",
    "            dataset = dataset.shuffle(len(X_i), seed=42)\n",
    "\n",
    "        with tf.io.TFRecordWriter(file_name) as writer:\n",
    "            for image, label in dataset:\n",
    "                example = create_example(image, label)\n",
    "                writer.write(example.SerializeToString())\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = write_record_new_version(X_train,y_train,'train')\n",
    "valid_path =  write_record_new_version(X_valid,y_valid,'valid')\n",
    "test_path =  write_record_new_version(X_test,y_test,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_1.tfrecord', 'train_2.tfrecord']\n",
      "['valid_1.tfrecord', 'valid_2.tfrecord']\n",
      "['test_1.tfrecord', 'test_2.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "print(train_path)\n",
    "print(valid_path)\n",
    "print(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]\n",
    "\n",
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n",
    "                  n_parse_threads=5, batch_size=32, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths,\n",
    "                                      num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_path, shuffle_buffer_size=60000)\n",
    "valid_set = mnist_dataset(valid_path)\n",
    "test_set = mnist_dataset(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "standardize = tf.keras.layers.Normalization(input_shape=[28,28])\n",
    "sample_image_batches = train_set.take(100).map(lambda image, label: image)\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n",
    "                               axis=0).astype(np.float32)\n",
    "standardize.adapt(sample_images)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    standardize,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 28, 28)            57        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79567 (310.81 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 57 (232.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 16s 7ms/step - loss: 5810.9912 - accuracy: 0.9263 - val_loss: 21253.4746 - val_accuracy: 0.9580\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 1156.2948 - accuracy: 0.9669 - val_loss: 23843.6406 - val_accuracy: 0.9660\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 2169.1736 - accuracy: 0.9771 - val_loss: 21175.2207 - val_accuracy: 0.9712\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 10s 5ms/step - loss: 65.4290 - accuracy: 0.9834 - val_loss: 25475.5801 - val_accuracy: 0.9704\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 1767.8092 - accuracy: 0.9863 - val_loss: 20648.5879 - val_accuracy: 0.9706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2760445bb10>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "logs = Path() / \"my_logs\" / \"run_\" / datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logs, histogram_freq=1, profile_batch=10)\n",
    "\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set,\n",
    "          callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('datasets/aclImdb')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = \"https://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "filename = \"aclImdb_v1.tar.gz\"\n",
    "filepath = tf.keras.utils.get_file(filename, root + filename, extract=True,\n",
    "                                   cache_dir=\".\")\n",
    "path = Path(filepath).with_name(\"aclImdb\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
    "\n",
    "train_pos = review_paths(path / \"train\" / \"pos\")\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_valid_neg = review_paths(path / \"test\" / \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_valid_pos)\n",
    "\n",
    "test_pos = test_valid_pos[:5000]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative): # first way of reading data from files\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.constant(reviews), tf.constant(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5): # second way of reading data from files\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000, seed=42)\n",
    "train_set = train_set.batch(batch_size).prefetch(1)\n",
    "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
    "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 1000\n",
    "sample_reviews = train_set.map(lambda review, label: review)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens, output_mode=\"tf_idf\")\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    780/Unknown - 15s 12ms/step - loss: 0.4258 - accuracy: 0.8239"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorization,\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Books\\BookCode\\DeepLearning\\Chapter13\\Exercises\\ch13.ipynb Cell 24\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter13/Exercises/ch13.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     sqrt_n_words \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msqrt(tf\u001b[39m.\u001b[39mcast(n_words, tf\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter13/Exercises/ch13.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mreduce_sum(inputs, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m sqrt_n_words\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter13/Exercises/ch13.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m another_example \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant([[[\u001b[39m1.\u001b[39m, \u001b[39m2.\u001b[39m, \u001b[39m3.\u001b[39m], [\u001b[39m4.\u001b[39m, \u001b[39m5.\u001b[39m, \u001b[39m0.\u001b[39m], [\u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m]],\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter13/Exercises/ch13.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                [[\u001b[39m6.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m], [\u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m], [\u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m]]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter13/Exercises/ch13.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m compute_mean_embedding(another_example)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n",
    "\n",
    "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
    "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
    "compute_mean_embedding(another_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens, output_mode=\"int\")\n",
    "text_vectorization.adapt(sample_reviews)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorization,\n",
    "    tf.keras.layers.Embedding(input_dim=max_tokens,\n",
    "                              output_dim=embedding_size,\n",
    "                              mask_zero=True), \n",
    "    tf.keras.layers.Lambda(compute_mean_embedding),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"imdb_reviews\")\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
