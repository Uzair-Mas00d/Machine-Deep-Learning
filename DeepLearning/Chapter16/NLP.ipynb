{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  \n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12], dtype=int64)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2  \n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  \n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Books\\BookCode\\DeepLearning\\Chapter16\\NLP.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnadam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model_ckpt \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmy_shakespeare_model\u001b[39m\u001b[39m\"\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_set, validation_data\u001b[39m=\u001b[39;49mvalid_set, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[model_ckpt])\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:890\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    889\u001b[0m     \u001b[39m# no_variable_creation function.\u001b[39;00m\n\u001b[1;32m--> 890\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    891\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    892\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    893\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_function_spec  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    894\u001b[0m       \u001b[39m.\u001b[39mcanonicalize_function_inputs(\n\u001b[0;32m    895\u001b[0m           args, kwds))\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)  \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "                    callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Books\\BookCode\\DeepLearning\\Chapter16\\NLP.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m path \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mget_file(\u001b[39m\"\u001b[39m\u001b[39mshakespeare_model.tgz\u001b[39m\u001b[39m\"\u001b[39m, url, extract\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_path \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mwith_name(\u001b[39m\"\u001b[39m\u001b[39mshakespeare_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m shakespeare_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(model_path)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[0;32m    231\u001b[0m         filepath,\n\u001b[0;32m    232\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[0;32m    233\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[0;32m    234\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[0;32m    235\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m    239\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    240\u001b[0m )\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:239\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m     )\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39;49mload(\n\u001b[0;32m    240\u001b[0m         filepath_str, \u001b[39mcompile\u001b[39;49m, options\n\u001b[0;32m    241\u001b[0m     )\n\u001b[0;32m    242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     \u001b[39mif\u001b[39;00m h5py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:145\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, compile, options)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m    142\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\n\u001b[0;32m    143\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrying to load ShardedVariables\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m     )\n\u001b[1;32m--> 145\u001b[0m     loaded \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49mload_partial(\n\u001b[0;32m    146\u001b[0m         path, nodes_to_load, options\u001b[39m=\u001b[39;49moptions\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    149\u001b[0m \u001b[39m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m keras_loader\u001b[39m.\u001b[39mfinalize_objects()\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:988\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39minit_scope():\n\u001b[0;32m    987\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 988\u001b[0m     loader \u001b[39m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    989\u001b[0m                     ckpt_options, options, filters)\n\u001b[0;32m    990\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    991\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m         \u001b[39mstr\u001b[39m(err) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m You may be trying to load on a different device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfrom the computational device. Consider setting the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    994\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto the io_device such as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m/job:localhost\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:161\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_proto \u001b[39m=\u001b[39m object_graph_proto\n\u001b[0;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_export_dir \u001b[39m=\u001b[39m export_dir\n\u001b[0;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_functions \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 161\u001b[0m     function_deserialization\u001b[39m.\u001b[39;49mload_function_def_library(\n\u001b[0;32m    162\u001b[0m         library\u001b[39m=\u001b[39;49mmeta_graph\u001b[39m.\u001b[39;49mgraph_def\u001b[39m.\u001b[39;49mlibrary,\n\u001b[0;32m    163\u001b[0m         saved_object_graph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_proto,\n\u001b[0;32m    164\u001b[0m         wrapper_function\u001b[39m=\u001b[39;49m_WrapperFunction))\n\u001b[0;32m    165\u001b[0m \u001b[39m# Store a set of all concrete functions that have been set up with\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39m# captures.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restored_concrete_functions \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py:422\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[1;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m# There is no need to copy all functions into the function def graph. It\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m# leads to a O(n^2) increase of memory when importing functions and the\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39m# extra function definitions are a no-op since they already imported as a\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39m# function before and passed in explicitly (due to the topologic sort\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[39m# import).\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m--> 422\u001b[0m   func_graph \u001b[39m=\u001b[39m function_def_lib\u001b[39m.\u001b[39;49mfunction_def_to_graph(\n\u001b[0;32m    423\u001b[0m       fdef,\n\u001b[0;32m    424\u001b[0m       structured_input_signature\u001b[39m=\u001b[39;49mstructured_input_signature,\n\u001b[0;32m    425\u001b[0m       structured_outputs\u001b[39m=\u001b[39;49mstructured_outputs)\n\u001b[0;32m    426\u001b[0m \u001b[39m# Restores gradients for function-call ops (not the same as ops that use\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[39m# custom gradients)\u001b[39;00m\n\u001b[0;32m    428\u001b[0m _restore_gradient_functions(func_graph, renamed_functions, loaded_gradients)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py:97\u001b[0m, in \u001b[0;36mfunction_def_to_graph\u001b[1;34m(fdef, structured_input_signature, structured_outputs, input_shapes, propagate_device_spec, include_library_functions)\u001b[0m\n\u001b[0;32m     91\u001b[0m graph_def, nested_to_flat_tensor_name \u001b[39m=\u001b[39m function_def_to_graph_def(\n\u001b[0;32m     92\u001b[0m     fdef, input_shapes, include_library_functions\u001b[39m=\u001b[39minclude_library_functions\n\u001b[0;32m     93\u001b[0m )\n\u001b[0;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m func_graph\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m     96\u001b[0m   \u001b[39m# Add all function nodes to the graph.\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m   importer\u001b[39m.\u001b[39;49mimport_graph_def_for_function(\n\u001b[0;32m     98\u001b[0m       graph_def, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, propagate_device_spec\u001b[39m=\u001b[39;49mpropagate_device_spec)\n\u001b[0;32m    100\u001b[0m   \u001b[39m# Initialize fields specific to FuncGraph.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m   \u001b[39m# inputs\u001b[39;00m\n\u001b[0;32m    103\u001b[0m   input_tensor_names \u001b[39m=\u001b[39m [\n\u001b[0;32m    104\u001b[0m       nested_to_flat_tensor_name[arg\u001b[39m.\u001b[39mname] \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m fdef\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39minput_arg\n\u001b[0;32m    105\u001b[0m   ]\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\importer.py:417\u001b[0m, in \u001b[0;36mimport_graph_def_for_function\u001b[1;34m(graph_def, name, propagate_device_spec)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimport_graph_def_for_function\u001b[39m(  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     graph_def, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, propagate_device_spec\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    416\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Like import_graph_def but does not validate colocation constraints.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m   \u001b[39mreturn\u001b[39;00m _import_graph_def_internal(\n\u001b[0;32m    418\u001b[0m       graph_def,\n\u001b[0;32m    419\u001b[0m       validate_colocation_constraints\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    420\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    421\u001b[0m       propagate_device_spec\u001b[39m=\u001b[39;49mpropagate_device_spec)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\importer.py:507\u001b[0m, in \u001b[0;36m_import_graph_def_internal\u001b[1;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list, propagate_device_spec)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[39m# _ProcessNewOps mutates the new operations. _mutation_lock ensures a\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39m# Session.run call cannot occur between creating the TF_Operations in the\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m# TF_GraphImportGraphDefWithResults call and mutating the them in\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# _ProcessNewOps.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39m_mutation_lock():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m   \u001b[39mwith\u001b[39;00m c_api_util\u001b[39m.\u001b[39mtf_buffer(graph_def\u001b[39m.\u001b[39;49mSerializeToString()) \u001b[39mas\u001b[39;00m serialized:\n\u001b[0;32m    508\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m       \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39m_c_graph\u001b[39m.\u001b[39mget() \u001b[39mas\u001b[39;00m c_graph:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1086\u001b[0m, in \u001b[0;36m_AddSerializeToStringMethod.<locals>.SerializeToString\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mIsInitialized():\n\u001b[0;32m   1083\u001b[0m   \u001b[39mraise\u001b[39;00m message_mod\u001b[39m.\u001b[39mEncodeError(\n\u001b[0;32m   1084\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mMessage \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is missing required fields: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[0;32m   1085\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDESCRIPTOR\u001b[39m.\u001b[39mfull_name, \u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFindInitializationErrors())))\n\u001b[1;32m-> 1086\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSerializePartialToString(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1095\u001b[0m, in \u001b[0;36m_AddSerializePartialToStringMethod.<locals>.SerializePartialToString\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mSerializePartialToString\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1094\u001b[0m   out \u001b[39m=\u001b[39m BytesIO()\n\u001b[1;32m-> 1095\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_InternalSerialize(out\u001b[39m.\u001b[39;49mwrite, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1096\u001b[0m   \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1115\u001b[0m, in \u001b[0;36m_AddSerializePartialToStringMethod.<locals>.InternalSerialize\u001b[1;34m(self, write_bytes, deterministic)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1114\u001b[0m   \u001b[39mfor\u001b[39;00m field_descriptor, field_value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mListFields():\n\u001b[1;32m-> 1115\u001b[0m     field_descriptor\u001b[39m.\u001b[39;49m_encoder(write_bytes, field_value, deterministic)\n\u001b[0;32m   1116\u001b[0m   \u001b[39mfor\u001b[39;00m tag_bytes, value_bytes \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unknown_fields:\n\u001b[0;32m   1117\u001b[0m     write_bytes(tag_bytes)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\encoder.py:761\u001b[0m, in \u001b[0;36mMessageEncoder.<locals>.EncodeRepeatedField\u001b[1;34m(write, value, deterministic)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m value:\n\u001b[0;32m    760\u001b[0m   write(tag)\n\u001b[1;32m--> 761\u001b[0m   local_EncodeVarint(write, element\u001b[39m.\u001b[39;49mByteSize(), deterministic)\n\u001b[0;32m    762\u001b[0m   element\u001b[39m.\u001b[39m_InternalSerialize(write, deterministic)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1065\u001b[0m, in \u001b[0;36m_AddByteSizeMethod.<locals>.ByteSize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1064\u001b[0m   \u001b[39mfor\u001b[39;00m field_descriptor, field_value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mListFields():\n\u001b[1;32m-> 1065\u001b[0m     size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m field_descriptor\u001b[39m.\u001b[39;49m_sizer(field_value)\n\u001b[0;32m   1066\u001b[0m   \u001b[39mfor\u001b[39;00m tag_bytes, value_bytes \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unknown_fields:\n\u001b[0;32m   1067\u001b[0m     size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tag_bytes) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(value_bytes)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\encoder.py:358\u001b[0m, in \u001b[0;36mMapSizer.<locals>.FieldSize\u001b[1;34m(map_value)\u001b[0m\n\u001b[0;32m    352\u001b[0m value \u001b[39m=\u001b[39m map_value[key]\n\u001b[0;32m    353\u001b[0m \u001b[39m# It's wasteful to create the messages and throw them away one second\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[39m# later since we'll do the same for the actual encode.  But there's not an\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[39m# obvious way to avoid this within the current design without tons of code\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39m# duplication. For message map, value.ByteSize() should be called to\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[39m# update the status.\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m entry_msg \u001b[39m=\u001b[39m message_type\u001b[39m.\u001b[39;49m_concrete_class(key\u001b[39m=\u001b[39;49mkey, value\u001b[39m=\u001b[39;49mvalue)\n\u001b[0;32m    359\u001b[0m total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m message_sizer(entry_msg)\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m is_message_map:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:550\u001b[0m, in \u001b[0;36m_AddInitMethod.<locals>.init\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m   new_val \u001b[39m=\u001b[39m field\u001b[39m.\u001b[39mmessage_type\u001b[39m.\u001b[39m_concrete_class(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfield_value)\n\u001b[0;32m    549\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m   copy\u001b[39m.\u001b[39;49mMergeFrom(new_val)\n\u001b[0;32m    551\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m   _ReraiseTypeErrorWithFieldName(message_descriptor\u001b[39m.\u001b[39mname, field_name)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1336\u001b[0m, in \u001b[0;36m_AddMergeFromMethod.<locals>.MergeFrom\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m   1334\u001b[0m       field_value \u001b[39m=\u001b[39m field\u001b[39m.\u001b[39m_default_constructor(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1335\u001b[0m       fields[field] \u001b[39m=\u001b[39m field_value\n\u001b[1;32m-> 1336\u001b[0m     field_value\u001b[39m.\u001b[39;49mMergeFrom(value)\n\u001b[0;32m   1337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fields[field] \u001b[39m=\u001b[39m value\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1328\u001b[0m, in \u001b[0;36m_AddMergeFromMethod.<locals>.MergeFrom\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     field_value \u001b[39m=\u001b[39m field\u001b[39m.\u001b[39m_default_constructor(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1327\u001b[0m     fields[field] \u001b[39m=\u001b[39m field_value\n\u001b[1;32m-> 1328\u001b[0m   field_value\u001b[39m.\u001b[39;49mMergeFrom(value)\n\u001b[0;32m   1329\u001b[0m \u001b[39melif\u001b[39;00m field\u001b[39m.\u001b[39mcpp_type \u001b[39m==\u001b[39m CPPTYPE_MESSAGE:\n\u001b[0;32m   1330\u001b[0m   \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m_is_present_in_parent:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\containers.py:185\u001b[0m, in \u001b[0;36mRepeatedScalarFieldContainer.MergeFrom\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mMergeFrom\u001b[39m(\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    180\u001b[0m     other: Union[\u001b[39m'\u001b[39m\u001b[39mRepeatedScalarFieldContainer[_T]\u001b[39m\u001b[39m'\u001b[39m, Iterable[_T]],\n\u001b[0;32m    181\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Appends the contents of another repeated field of the same type to this\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m  one. We do not check the types of the individual fields.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\u001b[39m.\u001b[39mextend(other)\n\u001b[0;32m    186\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_listener\u001b[39m.\u001b[39mModified()\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\google\\protobuf\\internal\\containers.py:95\u001b[0m, in \u001b[0;36mBaseContainer.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Retrieves item by the specified key.\"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m---> 95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m     96\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns the number of elements in the container.\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "url = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\" # extra part to download pre trained model.Beacuse above model take around 5 to 10 hour or 24 hour\n",
    "path = tf.keras.utils.get_file(\"shakespeare_model.tgz\", url, extract=True)\n",
    "model_path = Path(path).with_name(\"shakespeare_model\")\n",
    "shakespeare_model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  \n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]]) \n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset_for_stateful_rnn(sequence, length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "\n",
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],\n",
    "                                                 length)\n",
    "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42) \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\n",
    "                              batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_stateful_shakespeare_model\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n",
    "                    epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "tf.random.set_seed(42)\n",
    "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0\n",
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0\n",
      "Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Moun ...\n",
      "Label: 0\n",
      "This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful perf ...\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "for review, label in raw_train_set.take(4):\n",
    "    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n",
    "    print(\"Label:\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Books\\BookCode\\DeepLearning\\Chapter16\\NLP.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     text_vec_layer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mEmbedding(vocab_size, embed_size),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mGRU(\u001b[39m128\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnadam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_set, validation_data\u001b[39m=\u001b[39;49mvalid_set, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:890\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    889\u001b[0m     \u001b[39m# no_variable_creation function.\u001b[39;00m\n\u001b[1;32m--> 890\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    891\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    892\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    893\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_function_spec  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    894\u001b[0m       \u001b[39m.\u001b[39mcanonicalize_function_inputs(\n\u001b[0;32m    895\u001b[0m           args, kwds))\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:147\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m--> 147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:398\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[0;32m    396\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[1;32m--> 398\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[0;32m    399\u001b[0m     args, kwargs, func_graph)\n\u001b[0;32m    401\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:305\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[1;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[0;32m    304\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[1;32m--> 305\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m    306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m    307\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m    308\u001b[0m         args,\n\u001b[0;32m    309\u001b[0m         kwargs,\n\u001b[0;32m    310\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    311\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[0;32m    312\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m    313\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[0;32m    314\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m    316\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m    317\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1055\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1052\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m   1054\u001b[0m _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1055\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[0;32m   1057\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:597\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    594\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    595\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 597\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    598\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[39mreturn\u001b[39;00m api\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[0;32m     42\u001b[0m       original_func,\n\u001b[0;32m     43\u001b[0m       args,\n\u001b[0;32m     44\u001b[0m       kwargs,\n\u001b[0;32m     45\u001b[0m       options\u001b[39m=\u001b[39;49mconverter\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[0;32m     46\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     47\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[0;32m     48\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     49\u001b[0m       ))\n\u001b[0;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[1;32mC:\\Users\\LAPTOP~1\\AppData\\Local\\Temp\\__autograph_generated_fileh36bkm1e.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(step_function), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), ag__\u001b[39m.\u001b[39;49mld(iterator)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[0;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[1;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[0;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:460\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training.py:1322\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1318\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1319\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m     )\n\u001b[0;32m   1321\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1322\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1323\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1324\u001b[0m     outputs,\n\u001b[0;32m   1325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1326\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1327\u001b[0m )\n\u001b[0;32m   1328\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[0;32m   1669\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   1672\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 1673\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3250\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3248\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m   3249\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 3250\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4048\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   4046\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   4047\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m-> 4048\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training.py:1303\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1303\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1304\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\engine\\training.py:1084\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[0;32m   1083\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m-> 1084\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[0;32m   1085\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:543\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(\u001b[39mself\u001b[39m, loss, var_list, tape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    523\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \n\u001b[0;32m    525\u001b[0m \u001b[39m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39m      None\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradients(loss, var_list, tape)\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:276\u001b[0m, in \u001b[0;36m_BaseOptimizer.compute_gradients\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m callable(var_list):\n\u001b[0;32m    274\u001b[0m             var_list \u001b[39m=\u001b[39m var_list()\n\u001b[1;32m--> 276\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, var_list)\n\u001b[0;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1059\u001b[0m           output_gradients))\n\u001b[0;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   1065\u001b[0m     flat_targets,\n\u001b[0;32m   1066\u001b[0m     flat_sources,\n\u001b[0;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:336\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions._backward.<locals>._backward_function\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backward_function\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m    335\u001b[0m   call_op \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mop\n\u001b[1;32m--> 336\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rewrite_forward_and_call_backward(call_op, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:239\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions._rewrite_forward_and_call_backward\u001b[1;34m(self, op, *doutputs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_rewrite_forward_and_call_backward\u001b[39m(\u001b[39mself\u001b[39m, op, \u001b[39m*\u001b[39mdoutputs):\n\u001b[0;32m    238\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m   forward_function, backwards_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_backward(\u001b[39mlen\u001b[39;49m(doutputs))\n\u001b[0;32m    240\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m backwards_function\u001b[39m.\u001b[39moutputs:\n\u001b[0;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m backwards_function\u001b[39m.\u001b[39mstructured_outputs\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:172\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions.forward_backward\u001b[1;34m(self, num_doutputs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m forward_backward \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   \u001b[39mreturn\u001b[39;00m forward_backward\n\u001b[1;32m--> 172\u001b[0m forward, backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_construct_forward_backward(num_doutputs)\n\u001b[0;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_function_pairs[num_doutputs] \u001b[39m=\u001b[39m (forward, backward)\n\u001b[0;32m    174\u001b[0m \u001b[39mreturn\u001b[39;00m forward, backward\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:215\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions._construct_forward_backward\u001b[1;34m(self, num_doutputs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m    213\u001b[0m   backwards_graph \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39mFuncGraph(\n\u001b[0;32m    214\u001b[0m       _backward_name(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph\u001b[39m.\u001b[39mname))\n\u001b[1;32m--> 215\u001b[0m   func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m    216\u001b[0m       name\u001b[39m=\u001b[39;49mbackwards_graph\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    217\u001b[0m       python_func\u001b[39m=\u001b[39;49m_backprop_function,\n\u001b[0;32m    218\u001b[0m       args\u001b[39m=\u001b[39;49m[], kwargs\u001b[39m=\u001b[39;49m{},\n\u001b[0;32m    219\u001b[0m       signature\u001b[39m=\u001b[39;49msignature,\n\u001b[0;32m    220\u001b[0m       func_graph\u001b[39m=\u001b[39;49mbackwards_graph)\n\u001b[0;32m    221\u001b[0m   backwards_graph_captures \u001b[39m=\u001b[39m backwards_graph\u001b[39m.\u001b[39mexternal_captures\n\u001b[0;32m    222\u001b[0m   captures_from_forward \u001b[39m=\u001b[39m [\n\u001b[0;32m    223\u001b[0m       c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m backwards_graph_captures \u001b[39mif\u001b[39;00m\n\u001b[0;32m    224\u001b[0m       \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(c, ops\u001b[39m.\u001b[39mEagerTensor) \u001b[39mand\u001b[39;00m c\u001b[39m.\u001b[39mgraph \u001b[39mis\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph]\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1055\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1052\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m   1054\u001b[0m _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1055\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[0;32m   1057\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:206\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions._construct_forward_backward.<locals>._backprop_function\u001b[1;34m(*grad_ys)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backprop_function\u001b[39m(\u001b[39m*\u001b[39mgrad_ys):\n\u001b[0;32m    205\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mdevice(\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m gradients_util\u001b[39m.\u001b[39;49m_GradientsHelper(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    207\u001b[0m         trainable_outputs,\n\u001b[0;32m    208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph\u001b[39m.\u001b[39;49minputs,\n\u001b[0;32m    209\u001b[0m         grad_ys\u001b[39m=\u001b[39;49mgrad_ys,\n\u001b[0;32m    210\u001b[0m         src_graph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:687\u001b[0m, in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39mwith\u001b[39;00m src_graph\u001b[39m.\u001b[39m_original_op(op):\n\u001b[0;32m    683\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m    684\u001b[0m   \u001b[39mif\u001b[39;00m grad_fn:\n\u001b[0;32m    685\u001b[0m     \u001b[39m# If grad_fn was found, do not use SymbolicGradient even for\u001b[39;00m\n\u001b[0;32m    686\u001b[0m     \u001b[39m# functions.\u001b[39;00m\n\u001b[1;32m--> 687\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m    688\u001b[0m                              \u001b[39mlambda\u001b[39;49;00m: grad_fn(op, \u001b[39m*\u001b[39;49mout_grads))\n\u001b[0;32m    689\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     \u001b[39m# For function call ops, we add a 'SymbolicGradient'\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# node to the graph to compute gradients.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m    693\u001b[0m                              \u001b[39mlambda\u001b[39;00m: _SymGrad(op, out_grads))\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:327\u001b[0m, in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    324\u001b[0m     xla_compile \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m xla_compile:\n\u001b[1;32m--> 327\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn()  \u001b[39m# Exit early\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[39m# If the gradients are supposed to be compiled separately, we give them a\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[39m# _XlaScope name that is based on the name_scope of the gradients.  Otherwise\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39m# they just inherit the existing _XlaScope name, which lets them be merged\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m# together with the non-gradient computation.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m xla_separate_compiled_gradients:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:688\u001b[0m, in \u001b[0;36m_GradientsHelper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39mwith\u001b[39;00m src_graph\u001b[39m.\u001b[39m_original_op(op):\n\u001b[0;32m    683\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m    684\u001b[0m   \u001b[39mif\u001b[39;00m grad_fn:\n\u001b[0;32m    685\u001b[0m     \u001b[39m# If grad_fn was found, do not use SymbolicGradient even for\u001b[39;00m\n\u001b[0;32m    686\u001b[0m     \u001b[39m# functions.\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 688\u001b[0m                              \u001b[39mlambda\u001b[39;00m: grad_fn(op, \u001b[39m*\u001b[39;49mout_grads))\n\u001b[0;32m    689\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     \u001b[39m# For function call ops, we add a 'SymbolicGradient'\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# node to the graph to compute gradients.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m    693\u001b[0m                              \u001b[39mlambda\u001b[39;00m: _SymGrad(op, out_grads))\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\while_v2.py:368\u001b[0m, in \u001b[0;36m_WhileGrad\u001b[1;34m(op, *grads)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39m# We compute the gradient for the sub-graph between trainable ys and xs\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39m# with non-None incoming gradients. We later pad the None's to the list of\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39m# outputs.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m ys, xs, non_none_grads \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[(y, x, grad) \u001b[39mfor\u001b[39;00m (y, x, grad) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    366\u001b[0m     body_graph\u001b[39m.\u001b[39moutputs, body_graph\u001b[39m.\u001b[39minputs, grads) \u001b[39mif\u001b[39;00m grad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\n\u001b[1;32m--> 368\u001b[0m body_grad_graph, args \u001b[39m=\u001b[39m _create_grad_func(\n\u001b[0;32m    369\u001b[0m     ys, xs, non_none_grads, cond_graph, body_graph,\n\u001b[0;32m    370\u001b[0m     util\u001b[39m.\u001b[39;49munique_grad_fn_name(body_graph\u001b[39m.\u001b[39;49mname), op, maximum_iterations)\n\u001b[0;32m    372\u001b[0m \u001b[39mif\u001b[39;00m body_grad_graph\u001b[39m.\u001b[39mwhile_op_needs_rewrite:\n\u001b[0;32m    373\u001b[0m   \u001b[39m# Modify 'op' to output the intermediate accumulators needed by the grad\u001b[39;00m\n\u001b[0;32m    374\u001b[0m   \u001b[39m# function.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m   \u001b[39m# NOTE(skyewm): if there are any active sessions, this modification to `op`\u001b[39;00m\n\u001b[0;32m    376\u001b[0m   \u001b[39m# may make them unrunnable!\u001b[39;00m\n\u001b[0;32m    378\u001b[0m   cond_graph\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_rewritten\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\while_v2.py:664\u001b[0m, in \u001b[0;36m_create_grad_func\u001b[1;34m(ys, xs, grads, cond_graph, body_graph, name, while_op, maximum_iterations)\u001b[0m\n\u001b[0;32m    661\u001b[0m args \u001b[39m=\u001b[39m [counter, maximum_iterations, total_iters] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(grads)\n\u001b[0;32m    662\u001b[0m \u001b[39m# Note: The returned function does not have `args` in the list of\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m# `external_captures`.\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m grad_func_graph \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m    665\u001b[0m     name,\n\u001b[0;32m    666\u001b[0m     \u001b[39mlambda\u001b[39;49;00m \u001b[39m*\u001b[39;49margs: _grad_fn(ys, xs, args, body_graph),\n\u001b[0;32m    667\u001b[0m     args, {},\n\u001b[0;32m    668\u001b[0m     func_graph\u001b[39m=\u001b[39;49m_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\n\u001b[0;32m    669\u001b[0m                                        maximum_iterations, while_op,\n\u001b[0;32m    670\u001b[0m                                        body_graph_inputs, body_graph_outputs))\n\u001b[0;32m    672\u001b[0m \u001b[39m# Update the list of outputs with tensors corresponding to the captured\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[39m# tensors. We capture 3 types of tensors when building the grad fn:\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[39m# 1. Accumulators for forward graph intermediates which are not loop\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39m# 2. Resources, which are output as is.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39m# 3. Forward graph loop invariants, which are output as is.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[39mfor\u001b[39;00m external_capture, internal_capture \u001b[39min\u001b[39;00m grad_func_graph\u001b[39m.\u001b[39mcaptures:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1055\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1052\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m   1054\u001b[0m _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1055\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[0;32m   1057\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\while_v2.py:666\u001b[0m, in \u001b[0;36m_create_grad_func.<locals>.<lambda>\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    661\u001b[0m args \u001b[39m=\u001b[39m [counter, maximum_iterations, total_iters] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(grads)\n\u001b[0;32m    662\u001b[0m \u001b[39m# Note: The returned function does not have `args` in the list of\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m# `external_captures`.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m grad_func_graph \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    665\u001b[0m     name,\n\u001b[1;32m--> 666\u001b[0m     \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs: _grad_fn(ys, xs, args, body_graph),\n\u001b[0;32m    667\u001b[0m     args, {},\n\u001b[0;32m    668\u001b[0m     func_graph\u001b[39m=\u001b[39m_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\n\u001b[0;32m    669\u001b[0m                                        maximum_iterations, while_op,\n\u001b[0;32m    670\u001b[0m                                        body_graph_inputs, body_graph_outputs))\n\u001b[0;32m    672\u001b[0m \u001b[39m# Update the list of outputs with tensors corresponding to the captured\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[39m# tensors. We capture 3 types of tensors when building the grad fn:\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[39m# 1. Accumulators for forward graph intermediates which are not loop\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39m# 2. Resources, which are output as is.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39m# 3. Forward graph loop invariants, which are output as is.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[39mfor\u001b[39;00m external_capture, internal_capture \u001b[39min\u001b[39;00m grad_func_graph\u001b[39m.\u001b[39mcaptures:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\while_v2.py:721\u001b[0m, in \u001b[0;36m_grad_fn\u001b[1;34m(ys, xs, args, func_graph)\u001b[0m\n\u001b[0;32m    714\u001b[0m grad_ys \u001b[39m=\u001b[39m args[\u001b[39m3\u001b[39m:]\n\u001b[0;32m    716\u001b[0m \u001b[39m# Build the gradient graph. Note that this builds the gradient computation of\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \u001b[39m# func_graph in the current graph, which requires capturing tensors from\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[39m# func_graph. The captured func_graph tensors are resolved to external tensors\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[39m# after the forward While op has been rewritten in _resolve_grad_captures.\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[39m# TODO(srbs): Mark GradientsHelper as public?\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m grad_outs \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39;49m_GradientsHelper(\n\u001b[0;32m    722\u001b[0m     ys, xs, grad_ys\u001b[39m=\u001b[39;49mgrad_ys, src_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[0;32m    723\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzero\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    725\u001b[0m \u001b[39m# TODO(b/118712257): Handle the case when grad_outs has None's e.g. when there\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# is a tf.StopGradient in the loop body.\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grad_outs)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:687\u001b[0m, in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39mwith\u001b[39;00m src_graph\u001b[39m.\u001b[39m_original_op(op):\n\u001b[0;32m    683\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m    684\u001b[0m   \u001b[39mif\u001b[39;00m grad_fn:\n\u001b[0;32m    685\u001b[0m     \u001b[39m# If grad_fn was found, do not use SymbolicGradient even for\u001b[39;00m\n\u001b[0;32m    686\u001b[0m     \u001b[39m# functions.\u001b[39;00m\n\u001b[1;32m--> 687\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m    688\u001b[0m                              \u001b[39mlambda\u001b[39;49;00m: grad_fn(op, \u001b[39m*\u001b[39;49mout_grads))\n\u001b[0;32m    689\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     \u001b[39m# For function call ops, we add a 'SymbolicGradient'\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# node to the graph to compute gradients.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m    693\u001b[0m                              \u001b[39mlambda\u001b[39;00m: _SymGrad(op, out_grads))\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:327\u001b[0m, in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    324\u001b[0m     xla_compile \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m xla_compile:\n\u001b[1;32m--> 327\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn()  \u001b[39m# Exit early\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[39m# If the gradients are supposed to be compiled separately, we give them a\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[39m# _XlaScope name that is based on the name_scope of the gradients.  Otherwise\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39m# they just inherit the existing _XlaScope name, which lets them be merged\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m# together with the non-gradient computation.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m xla_separate_compiled_gradients:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:688\u001b[0m, in \u001b[0;36m_GradientsHelper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39mwith\u001b[39;00m src_graph\u001b[39m.\u001b[39m_original_op(op):\n\u001b[0;32m    683\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m    684\u001b[0m   \u001b[39mif\u001b[39;00m grad_fn:\n\u001b[0;32m    685\u001b[0m     \u001b[39m# If grad_fn was found, do not use SymbolicGradient even for\u001b[39;00m\n\u001b[0;32m    686\u001b[0m     \u001b[39m# functions.\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 688\u001b[0m                              \u001b[39mlambda\u001b[39;00m: grad_fn(op, \u001b[39m*\u001b[39;49mout_grads))\n\u001b[0;32m    689\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     \u001b[39m# For function call ops, we add a 'SymbolicGradient'\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# node to the graph to compute gradients.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     in_grads \u001b[39m=\u001b[39m _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m    693\u001b[0m                              \u001b[39mlambda\u001b[39;00m: _SymGrad(op, out_grads))\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1736\u001b[0m, in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1734\u001b[0m b \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mconj(op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m])\n\u001b[0;32m   1735\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m t_b:\n\u001b[1;32m-> 1736\u001b[0m   grad_a \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(grad, b, transpose_b\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1737\u001b[0m   grad_b \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmat_mul(a, grad, transpose_a\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1738\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m t_b:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6735\u001b[0m, in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6733\u001b[0m   transpose_b \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   6734\u001b[0m transpose_b \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39mmake_bool(transpose_b, \u001b[39m\"\u001b[39m\u001b[39mtranspose_b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 6735\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[0;32m   6736\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, a\u001b[39m=\u001b[39;49ma, b\u001b[39m=\u001b[39;49mb, transpose_a\u001b[39m=\u001b[39;49mtranspose_a, transpose_b\u001b[39m=\u001b[39;49mtranspose_b,\n\u001b[0;32m   6737\u001b[0m                 name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   6738\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[0;32m   6739\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:795\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    790\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    791\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[0;32m    792\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    793\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    794\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 795\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    796\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m    797\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m    799\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[0;32m    803\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\while_v2.py:1012\u001b[0m, in \u001b[0;36m_WhileBodyGradFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[39mif\u001b[39;00m (op_type \u001b[39min\u001b[39;00m optimized_reduction_ops \u001b[39mand\u001b[39;00m\n\u001b[0;32m    996\u001b[0m     \u001b[39mnot\u001b[39;00m util\u001b[39m.\u001b[39moutput_all_intermediates() \u001b[39mand\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[39mall\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mgraph \u001b[39mis\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m inputs) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1000\u001b[0m     \u001b[39mnot\u001b[39;00m util\u001b[39m.\u001b[39mgraph_wrapped_for_higher_order_tape_gradients(\n\u001b[0;32m   1001\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph)):\n\u001b[0;32m   1002\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_move_op_to_forward_graph(\n\u001b[0;32m   1003\u001b[0m       op_type,\n\u001b[0;32m   1004\u001b[0m       inputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1009\u001b[0m       op_def\u001b[39m=\u001b[39mop_def,\n\u001b[0;32m   1010\u001b[0m       compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[1;32m-> 1012\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_WhileBodyGradFuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(\n\u001b[0;32m   1013\u001b[0m     op_type,\n\u001b[0;32m   1014\u001b[0m     inputs,\n\u001b[0;32m   1015\u001b[0m     dtypes\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   1016\u001b[0m     input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   1017\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1018\u001b[0m     attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1019\u001b[0m     op_def\u001b[39m=\u001b[39;49mop_def,\n\u001b[0;32m   1020\u001b[0m     compute_device\u001b[39m=\u001b[39;49mcompute_device)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:668\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    666\u001b[0m   \u001b[39mif\u001b[39;00m ctxt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ctxt, \u001b[39m\"\u001b[39m\u001b[39mAddValue\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    667\u001b[0m     inp \u001b[39m=\u001b[39m ctxt\u001b[39m.\u001b[39mAddValue(inp)\n\u001b[1;32m--> 668\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[0;32m    670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:675\u001b[0m, in \u001b[0;36mFuncGraph.capture\u001b[1;34m(self, tensor, name, shape)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcapture\u001b[39m(\u001b[39mself\u001b[39m, tensor, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 675\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_captures\u001b[39m.\u001b[39;49mcapture_by_value(\u001b[39mself\u001b[39;49m, tensor, name)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\core\\function\\capture\\capture_container.py:150\u001b[0m, in \u001b[0;36mFunctionCaptures.capture_by_value\u001b[1;34m(self, graph, tensor, name)\u001b[0m\n\u001b[0;32m    147\u001b[0m     name \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mname\n\u001b[0;32m    148\u001b[0m   \u001b[39m# cond/while graphs override _capture_helper() so cannot call\u001b[39;00m\n\u001b[0;32m    149\u001b[0m   \u001b[39m# self.create_placeholder_helper() here directly.\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m graph\u001b[39m.\u001b[39;49m_capture_helper(tensor, name)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\while_v2.py:1132\u001b[0m, in \u001b[0;36m_WhileBodyGradFuncGraph._capture_helper\u001b[1;34m(self, tensor, name)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# No need to accumulate loop invariants. Capture them directly.\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# The captured tensor gets resolved to the corresponding while output in\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39m# `_resolve_grad_captures`.\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m _is_loop_invariant(tensor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph\u001b[39m.\u001b[39minputs,\n\u001b[0;32m   1130\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph\u001b[39m.\u001b[39moutputs):\n\u001b[0;32m   1131\u001b[0m   captured_tensor \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(_WhileBodyGradFuncGraph,\n\u001b[1;32m-> 1132\u001b[0m                           \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_capture_helper(tensor, name)\n\u001b[0;32m   1133\u001b[0m   \u001b[39m# Add to `internal_capture_to_output` so that this gets added to the list\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m   \u001b[39m# of outputs.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minternal_capture_to_output[ops\u001b[39m.\u001b[39mtensor_id(\n\u001b[0;32m   1136\u001b[0m       captured_tensor)] \u001b[39m=\u001b[39m captured_tensor\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:706\u001b[0m, in \u001b[0;36mFuncGraph._capture_helper\u001b[1;34m(self, tensor, name)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_capture_helper\u001b[39m(\u001b[39mself\u001b[39m, tensor, name):\n\u001b[1;32m--> 706\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_captures\u001b[39m.\u001b[39;49m_create_placeholder_helper(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    707\u001b[0m       \u001b[39mself\u001b[39;49m, tensor, name)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\core\\function\\capture\\capture_container.py:279\u001b[0m, in \u001b[0;36mFunctionCaptures._create_placeholder_helper\u001b[1;34m(self, graph, tensor, name)\u001b[0m\n\u001b[0;32m    274\u001b[0m   composite_device_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    275\u001b[0m placeholder_ctx \u001b[39m=\u001b[39m trace_type\u001b[39m.\u001b[39mInternalPlaceholderContext(\n\u001b[0;32m    276\u001b[0m     graph,\n\u001b[0;32m    277\u001b[0m     with_none_control_dependencies\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    278\u001b[0m     composite_device_name\u001b[39m=\u001b[39mcomposite_device_name)\n\u001b[1;32m--> 279\u001b[0m placeholder \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39;49mplaceholder_value(placeholder_ctx)\n\u001b[0;32m    280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_or_replace(\n\u001b[0;32m    281\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mid\u001b[39m(tensor),\n\u001b[0;32m    282\u001b[0m     external\u001b[39m=\u001b[39mtensor,\n\u001b[0;32m    283\u001b[0m     internal\u001b[39m=\u001b[39mplaceholder,\n\u001b[0;32m    284\u001b[0m     is_by_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    285\u001b[0m graph\u001b[39m.\u001b[39minputs\u001b[39m.\u001b[39mappend(placeholder)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:269\u001b[0m, in \u001b[0;36mTensorSpec.placeholder_value\u001b[1;34m(self, placeholder_context)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mif\u001b[39;00m placeholder_context\u001b[39m.\u001b[39mwith_none_control_dependencies:\n\u001b[0;32m    266\u001b[0m   \u001b[39m# Note: setting ops.control_dependencies(None) ensures we always put\u001b[39;00m\n\u001b[0;32m    267\u001b[0m   \u001b[39m# capturing placeholders outside of any control flow context.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m   \u001b[39mwith\u001b[39;00m context_graph\u001b[39m.\u001b[39mcontrol_dependencies(\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 269\u001b[0m     placeholder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph_placeholder(context_graph, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    270\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m   placeholder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_placeholder(context_graph, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:309\u001b[0m, in \u001b[0;36mTensorSpec._graph_placeholder\u001b[1;34m(self, graph, name)\u001b[0m\n\u001b[0;32m    307\u001b[0m attrs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype_value, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m: shape}\n\u001b[0;32m    308\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 309\u001b[0m   op \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    310\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mPlaceholder\u001b[39;49m\u001b[39m\"\u001b[39;49m, [], [dtype], input_types\u001b[39m=\u001b[39;49m[],\n\u001b[0;32m    311\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    312\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    313\u001b[0m   \u001b[39m# TODO(b/262413656) Sometimes parameter names are not valid op names, in\u001b[39;00m\n\u001b[0;32m    314\u001b[0m   \u001b[39m# which case an unnamed placeholder is created instead. Update this logic\u001b[39;00m\n\u001b[0;32m    315\u001b[0m   \u001b[39m# to sanitize the name instead of falling back on unnamed placeholders.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m   logging\u001b[39m.\u001b[39mwarning(e)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\ops\\while_v2.py:1012\u001b[0m, in \u001b[0;36m_WhileBodyGradFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[39mif\u001b[39;00m (op_type \u001b[39min\u001b[39;00m optimized_reduction_ops \u001b[39mand\u001b[39;00m\n\u001b[0;32m    996\u001b[0m     \u001b[39mnot\u001b[39;00m util\u001b[39m.\u001b[39moutput_all_intermediates() \u001b[39mand\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[39mall\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mgraph \u001b[39mis\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m inputs) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1000\u001b[0m     \u001b[39mnot\u001b[39;00m util\u001b[39m.\u001b[39mgraph_wrapped_for_higher_order_tape_gradients(\n\u001b[0;32m   1001\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_graph)):\n\u001b[0;32m   1002\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_move_op_to_forward_graph(\n\u001b[0;32m   1003\u001b[0m       op_type,\n\u001b[0;32m   1004\u001b[0m       inputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1009\u001b[0m       op_def\u001b[39m=\u001b[39mop_def,\n\u001b[0;32m   1010\u001b[0m       compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[1;32m-> 1012\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_WhileBodyGradFuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(\n\u001b[0;32m   1013\u001b[0m     op_type,\n\u001b[0;32m   1014\u001b[0m     inputs,\n\u001b[0;32m   1015\u001b[0m     dtypes\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   1016\u001b[0m     input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   1017\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1018\u001b[0m     attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1019\u001b[0m     op_def\u001b[39m=\u001b[39;49mop_def,\n\u001b[0;32m   1020\u001b[0m     compute_device\u001b[39m=\u001b[39;49mcompute_device)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3381\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3378\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3379\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3380\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3381\u001b[0m   ret \u001b[39m=\u001b[39m Operation\u001b[39m.\u001b[39;49mfrom_node_def(\n\u001b[0;32m   3382\u001b[0m       node_def,\n\u001b[0;32m   3383\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3384\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m   3385\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   3386\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[0;32m   3387\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   3388\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[0;32m   3389\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def,\n\u001b[0;32m   3390\u001b[0m   )\n\u001b[0;32m   3391\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   3392\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1889\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1886\u001b[0m     control_input_ops\u001b[39m.\u001b[39mappend(control_op)\n\u001b[0;32m   1888\u001b[0m \u001b[39m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1889\u001b[0m c_op \u001b[39m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   1890\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Operation(c_op, GraphTensor)\n\u001b[0;32m   1891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init(g)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1721\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1719\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39m_c_graph\u001b[39m.\u001b[39mget() \u001b[39mas\u001b[39;00m c_graph:\n\u001b[1;32m-> 1721\u001b[0m   op_desc \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_NewOperation(c_graph,\n\u001b[0;32m   1722\u001b[0m                                               compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mop),\n\u001b[0;32m   1723\u001b[0m                                               compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mname))\n\u001b[0;32m   1724\u001b[0m \u001b[39mif\u001b[39;00m node_def\u001b[39m.\u001b[39mdevice:\n\u001b[0;32m   1725\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  \n",
    "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "token_ids = text_vec_layer(inputs)\n",
    "mask = tf.math.not_equal(token_ids, 0)\n",
    "Z = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
    "Z = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[86, 18], [11, 7, 1, 116, 217]]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_ragged = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size, ragged=True)\n",
    "text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\n",
    "text_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\n",
    "tf.random.set_seed(42)  \n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                   trainable=True, dtype=tf.string, input_shape=[]),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Books\\BookCode\\DeepLearning\\Chapter16\\NLP.ipynb Cell 33\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m path \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mget_file(\u001b[39m\"\u001b[39m\u001b[39mspa-eng.zip\u001b[39m\u001b[39m\"\u001b[39m, origin\u001b[39m=\u001b[39murl, cache_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m, extract\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m text \u001b[39m=\u001b[39m (Path(path)\u001b[39m.\u001b[39mwith_name(\u001b[39m\"\u001b[39m\u001b[39mspa-eng\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mspa.txt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mread_text(encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True)\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text(encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Books\\BookCode\\DeepLearning\\Chapter16\\NLP.ipynb Cell 34\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m¡\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m¿\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pairs \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m text\u001b[39m.\u001b[39msplitlines()]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Books/BookCode/DeepLearning/Chapter16/NLP.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m42\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.seed(42) \n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How boring! => Qué aburrimiento!\n",
      "I love sports. => Adoro el deporte.\n",
      "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_es.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42) \n",
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en])  \n",
    "        X_dec = np.array([\"startofseq \" + translation])  \n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]  \n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I like soccer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I like soccer and also going to the beach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42) \n",
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_state=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  \n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I like soccer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42) \n",
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  \n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]  \n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = tf.keras.layers.Attention()\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(attention_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I like soccer and also going to the beach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50 \n",
    "embed_size = 128\n",
    "tf.random.set_seed(42) \n",
    "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
    "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
    "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
    "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
    "        p, i = np.meshgrid(np.arange(max_length),\n",
    "                           2 * np.arange(embed_size // 2))\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :batch_max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2  \n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "n_units = 128  \n",
    "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
    "Z = encoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "causal_mask = tf.linalg.band_part( \n",
    "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = Z  \n",
    "Z = decoder_in  \n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I like soccer and also going to the beach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae1aa1b7a4f4719944537a6f0ead53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Laptop House\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2c9d1d57d144059e478d8e5a03c896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d6b141efbf4798aa1fbaf33a1910ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4491cdf048480eab51d15f84452dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"The actors were very convincing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9896161556243896},\n",
       " {'label': 'NEGATIVE', 'score': 0.9811071157455444}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I am from India.\", \"I am from Iraq.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c81e1bf4494b4cbe752e72e4879337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874e45ffbd2140c29cd58476698e82d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94232ce0648640e2be8f3116a30351b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/58.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bee2bc94e94782b91b5333f5425def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf1c8bfaaf24794a213ced5a3ef0c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e1565194944ff99f97b318c43a9583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'contradiction', 'score': 0.9790192246437073}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
    "classifier_mnli(\"She loves me. [SEP] She loves me not.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
       "         102,    0,    0,    0],\n",
       "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
       "        2003, 2214, 1012,  102]])>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
    "                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n",
    "                      padding=True, return_tensors=\"tf\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
       "         102,    0,    0,    0],\n",
       "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
       "        2003, 2214, 1012,  102]])>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer([(\"I like soccer.\", \"We all love soccer!\"),\n",
    "                       (\"Joe lived for a very long time.\", \"Joe is old.\")],\n",
    "                      padding=True, return_tensors=\"tf\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-2.1123827 ,  1.1786776 ,  1.4101034 ],\n",
       "       [-0.01478252,  1.0962467 , -0.9919958 ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(token_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.01619699, 0.4352348 , 0.5485682 ],\n",
       "       [0.22656001, 0.6881722 , 0.08526777]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probas = tf.keras.activations.softmax(outputs.logits)\n",
    "Y_probas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1], dtype=int64)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = tf.argmax(Y_probas, axis=1)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 47s 47s/step - loss: 0.6666 - accuracy: 0.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3440 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "sentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\n",
    "X_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
    "y_train = tf.constant([0, 2])  \n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
